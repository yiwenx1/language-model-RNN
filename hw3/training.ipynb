{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tests import test_prediction, test_generation\n",
    "from config import LANGUAGEMODEL_CONFIG as LC\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seq_len': 200, 'lr': 0.01, 'nepochs': 3, 'batch_size': 16, 'embed_size': 256, 'hidden_size': 256, 'nlayers': 1}\n"
     ]
    }
   ],
   "source": [
    "print(LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(579,) articles in training set\n",
      "(33278,) vocabulary items\n"
     ]
    }
   ],
   "source": [
    "# load all that we need\n",
    "\n",
    "dataset = np.load('../dataset/wiki.train.npy')\n",
    "fixtures_pred = np.load('../fixtures/dev_fixtures/prediction.npz')  # dev\n",
    "fixtures_gen = np.load('../fixtures/dev_fixtures/generation.npy')  # dev\n",
    "fixtures_pred_test = np.load('../fixtures/test_fixtures/prediction.npz')  # test\n",
    "fixtures_gen_test = np.load('../fixtures/test_fixtures/generation.npy')  # test\n",
    "vocab = np.load('../dataset/vocab.npy')\n",
    "print(\"{} articles in training set\".format(dataset.shape))\n",
    "print(\"{} vocabulary items\".format(vocab.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_len=LC['seq_len']):\n",
    "        n_seq = len(text) // seq_len\n",
    "        text = text[:n_seq * seq_len]\n",
    "        text.astype(np.int32)\n",
    "        self.data = torch.tensor(text).view(-1,seq_len)\n",
    "    def __getitem__(self, i):\n",
    "        txt = self.data[i]\n",
    "        return txt[:-1], txt[1:]\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "def collate(seq_list):\n",
    "    \"\"\"\n",
    "    Transform a list of sequences into a batch.\n",
    "    Return dimension seq_len * batch_size\n",
    "    \"\"\"\n",
    "    inputs = torch.cat([s[0].unsqueeze(1) for s in seq_list], dim=1)\n",
    "    targets = torch.cat([s[1].unsqueeze(1) for s in seq_list], dim=1)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of one long string generated from the articles  (2075677,)\n"
     ]
    }
   ],
   "source": [
    "# Randomly shuffle the articles in the training dataset\n",
    "new_indices = np.random.permutation(len(dataset))\n",
    "new_dataset = np.take(dataset, new_indices)\n",
    "# Concatenate the articles into one long string, length N\n",
    "dataset_flatten = np.concatenate(new_dataset).ravel()\n",
    "print(\"length of one long string generated from the articles \", dataset_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "class LanguageModelDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        \n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        # concatenate your articles and build into batches\n",
    "        \n",
    "        raise NotImplemented\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "        TODO: Define your model here\n",
    "    \"\"\"\n",
    "    def __init__(self, charcount):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.vocab_size = charcount\n",
    "        self.embed_size = LC['embed_size']\n",
    "        self.hidden_size = LC['hidden_size']\n",
    "        self.nlayers = LC['nlayers']\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.rnn = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size, num_layers=self.nlayers)\n",
    "        self.scoring = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, seq_batch): # dimension of seq_batch: seq_len * batch_size\n",
    "        embed = self.embedding(seq_batch) # seq_len * batch_size * embed_size\n",
    "        hidden = None\n",
    "        output_lstm, hidden = self.rnn(embed, hidden) # seq_len * batch_size * hidden_size\n",
    "        output_lstm_flatten = output_lstm.view(-1, self.hidden_size) # (seq_len * batch_size) * hidden_size\n",
    "        output_flatten = self.scoring(output_lstm_flatten) # (seq_len * batch_size) * vocab_size\n",
    "        return output_flatten.view(-1,len(seq_batch),self.vocab_size) # seq_len * batch_size * vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trainer\n",
    "\n",
    "class LanguageModelTrainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # TODO: Define your optimizer and criterion here\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=LC['lr'], weight_decay=1e-6)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        \"\"\" \n",
    "            TODO: Define code for training a single batch of inputs\n",
    "        \n",
    "        \"\"\"\n",
    "        model.zero_grad()\n",
    "        inputs = inputs.long()\n",
    "        targets = targets.long()\n",
    "        outputs = model(inputs)\n",
    "        loss = self.criterion(outputs.view(-1,outputs.size(2)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def test(self):\n",
    "        # don't change these\n",
    "        self.model.eval() # set to eval mode\n",
    "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
    "        self.predictions.append(predictions)\n",
    "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        \n",
    "        generated_logits = TestLanguageModel.generation(fixtures_gen, 20, self.model) # predictions for 20 words\n",
    "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 20, self.model) # predictions for 20 words\n",
    "\n",
    "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "        self.val_losses.append(nll)\n",
    "        \n",
    "        self.generated.append(generated)\n",
    "        self.generated_test.append(generated_test)\n",
    "        self.generated_logits.append(generated_logits)\n",
    "        self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "        # generate predictions for test data\n",
    "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
    "        self.predictions_test.append(predictions_test)\n",
    "            \n",
    "        print('[VAL]  Epoch [%d/%d]   NLL: %.4f'\n",
    "                      % (self.epochs, self.max_epochs, nll))\n",
    "        return nll\n",
    "\n",
    "    def save(self):\n",
    "        # don't change these\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()},\n",
    "            model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-test-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated_test[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLanguageModel:\n",
    "    def prediction(inp, model):\n",
    "        \"\"\"\n",
    "            TODO: write prediction code here\n",
    "            \n",
    "            :param inp:\n",
    "            :return: a np.ndarray of logits\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "        \n",
    "    def generation(inp, forward, model):\n",
    "        \"\"\"\n",
    "            TODO: write generation code here\n",
    "\n",
    "            Generate a sequence of words given a starting sequence.\n",
    "            :param inp: Initial sequence of words (batch size, length)\n",
    "            :param forward: number of additional words to generate\n",
    "            :return: generated words (batch size, forward)\n",
    "        \"\"\"        \n",
    "        raise NotImplemented\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: define other hyperparameters here\n",
    "\n",
    "NUM_EPOCHS = LC['nepochs']\n",
    "BATCH_SIZE = LC['batch_size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./experiments/1540598901\n"
     ]
    }
   ],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LanguageModel(len(vocab))\n",
    "model = model.to(DEVICE)\n",
    "loader = DataLoader(dataset=TextDataset(dataset_flatten), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_nll = 1e30  # set to super large value at first\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \" + \n",
    "              str(epoch)+\" with NLL: \" + str(best_nll))\n",
    "        trainer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change these\n",
    "# plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation NLL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see generated output\n",
    "print (trainer.generated[-1]) # get last generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
